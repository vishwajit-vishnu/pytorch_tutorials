{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip3 install transformers"
      ],
      "metadata": {
        "id": "25guMTSWgckL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "qVO6sQOOgPiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"data/train_data_clean.csv\", header=None, encoding = \"ISO-8859-1\")\n",
        "train_sentiment_df = train_df.iloc[:, 0]\n",
        "\n",
        "res = train_sentiment_df.value_counts()\n",
        "print(f\"sentiment counts: \\n{res}\")\n"
      ],
      "metadata": {
        "id": "lPi-r1OrVi-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSM0iq2tf5P2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_path=\"data/test_data.csv\",\n",
        "                 sentiment_column_index=0, text_column_index=5):\n",
        "        super().__init__()\n",
        "        self.df = pd.read_csv(data_path, header=None, encoding=\"ISO-8859-1\")\n",
        "        self.sentiment_column_index = sentiment_column_index\n",
        "        self.text_column_index = text_column_index\n",
        "        self.sentiment_map = {\n",
        "            0: 0,  # Negative sentiment\n",
        "            2: 1,  # Neutral sentiment # absent in train dataset\n",
        "            4: 2,  # Positive sentiment\n",
        "        }\n",
        "        self.idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def generate_item(self, idx):\n",
        "        try:\n",
        "            input_text = str(self.df.iloc[idx, self.text_column_index])\n",
        "            sentiment = self.sentiment_map[\n",
        "                self.df.iloc[idx, self.sentiment_column_index]\n",
        "            ]\n",
        "\n",
        "            # clean input_text by removing user_name at beginning of sentence\n",
        "            input_text = input_text.strip()\n",
        "            if input_text.startswith(\"@\"):\n",
        "                first_space = input_text.find(\" \")\n",
        "                if first_space == -1:  # It means nothing  else is there apart from username\n",
        "                    raise Exception(\"Invalid input_text\")\n",
        "                input_text = input_text[first_space].strip()\n",
        "\n",
        "        except Exception as ex:\n",
        "            return None, None\n",
        "\n",
        "        return input_text, sentiment\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.idx = idx\n",
        "        input_text = None\n",
        "        sentiment = None\n",
        "        # check if input_text is empty or None and sentiment is None\n",
        "        while (not (input_text) or sentiment is None):\n",
        "            input_text, sentiment = self.generate_item(self.idx)\n",
        "            self.idx = self.idx + 1\n",
        "\n",
        "        # print(\"HERE: \", input_text, sentiment)\n",
        "        return str(input_text), sentiment\n",
        "\n",
        "\n",
        "train_batch_size = 128\n",
        "test_batch_size = 1\n",
        "val_batch_size = 64\n",
        "\n",
        "train_dataset = MyDataset(\n",
        "    data_path=\"data/train_data_clean.csv\",\n",
        "    sentiment_column_index=0, text_column_index=5\n",
        ")\n",
        "print(\"Entire train dataset length before creating val set: \", len(train_dataset))\n",
        "# splitting dataset to create validation split\n",
        "train_dataset, val_dataset = random_split(train_dataset, [0.99, 0.01],\n",
        "                                          generator=torch.Generator().manual_seed(42)\n",
        "                                          )  # 5% data as val set\n",
        "print(f\"Train data len: {len(train_dataset)}, Val data len: {len(val_dataset)}\")\n",
        "\n",
        "test_dataset = MyDataset(\n",
        "    data_path=\"data/test_data.csv\",\n",
        "    sentiment_column_index=0, text_column_index=5,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=train_batch_size, shuffle=True\n",
        "                          )\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=val_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=test_batch_size, shuffle=True\n",
        "                         )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7S9lHhqpYQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_classification_report(actual_sentiment, pred_sentiment):\n",
        "    # class_names = [\"pos\", \"neutral\", \"neg\"]\n",
        "    return classification_report(actual_sentiment, pred_sentiment)\n",
        "\n",
        "\n",
        "def plot_data(actual_sentiment, pred_sentiment):\n",
        "    cm = confusion_matrix(actual_sentiment, pred_sentiment)\n",
        "    disp = ConfusionMatrixDisplay(cm)\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(val_loss_logs, train_loss_logs, log_interval=50):\n",
        "    plt.plot([(x + 1) * log_interval for x in range(len(val_loss_logs))], val_loss_logs, label=\"validation\")\n",
        "    plt.plot([x for x in range(len(train_loss_logs))], train_loss_logs, label=\"train\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def soft_prob_assign_based_on_threshold(output, classification_threshold):\n",
        "    probs = torch.exp(output)\n",
        "\n",
        "    # # check if probabilities of both classes are less than 0.7 then classify as normal class\n",
        "    prob_more_than_thres = torch.where(probs > classification_threshold, 1.0, 0.0)\n",
        "    #  prob_more_than_thres will have [0,0,0] if all none prob is more than thres\n",
        "\n",
        "    correctly_classified = torch.any(prob_more_than_thres.bool(), dim=-1)  # [False]\n",
        "    # if not correctly classified then assign it to class 1(i.e. neutral class)\n",
        "    prob_more_than_thres[correctly_classified == False] = torch.tensor([0.1, 0.8, 0.1])\n",
        "\n",
        "    res = prob_more_than_thres\n",
        "    return res\n",
        "\n",
        "\n",
        "def train(model, optimizer, scheduler, train_loader, val_loader,\n",
        "          lr=0.00025, warmup_step=100,\n",
        "          classification_threshold=0.7, log_interval=50):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    prev_acc_score = -2.0\n",
        "    prev_val_loss = 0.333  # 0.424\n",
        "    val_loss_logs = []\n",
        "    train_loss_logs = []\n",
        "    loss = None\n",
        "    train_step = 0\n",
        "    while True:\n",
        "        pred_sentiment = []\n",
        "        actual_sentiment = []\n",
        "        for input_text, sentiment in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(list(input_text))\n",
        "\n",
        "            loss = F.nll_loss(output, sentiment)\n",
        "\n",
        "\n",
        "            # store label to calculate accuracy score\n",
        "            current_pred_sentiment = output.argmax(dim=-1)\n",
        "\n",
        "            # print(current_pred_sentiment.shape, current_pred_sentiment)\n",
        "            pred_sentiment += list(current_pred_sentiment.detach().numpy())\n",
        "            actual_sentiment += list(sentiment.detach().numpy())\n",
        "            train_loss_logs.append(float(loss))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_step += 1\n",
        "            if scheduler is not None:\n",
        "                # linear warmup stage\n",
        "                if train_step <= warmup_step:\n",
        "                    curr_lr = lr * train_step / warmup_step\n",
        "                    optimizer.param_groups[0]['lr'] = curr_lr;\n",
        "\n",
        "                scheduler.step();\n",
        "                # scheduler.step(train_step)\n",
        "\n",
        "            if train_step % log_interval == 0:\n",
        "                # Negative log likelihood loss logging for val_dataset\n",
        "                _, val_loss, _ = eval(model=model, test_loader=val_loader,\n",
        "                                      classification_threshold=classification_threshold)\n",
        "                val_loss_logs.append(float(val_loss))\n",
        "                print(f\"train_step: {train_step} lr: {optimizer.param_groups[0]['lr']}, \"\n",
        "                      f\"train_loss: {float(loss)}, val_loss: {float(val_loss)}\")\n",
        "\n",
        "                if val_loss < prev_val_loss:\n",
        "                    # saving model\n",
        "                    torch.save(model.state_dict(), \"model.pt\")\n",
        "                    torch.save(optimizer.state_dict(), \"optim.pt\")\n",
        "\n",
        "                if prev_val_loss - val_loss < 0.005:\n",
        "                    break\n",
        "\n",
        "                prev_acc_score = val_loss\n",
        "\n",
        "            # break\n",
        "        acc_score = accuracy_score(actual_sentiment, pred_sentiment)\n",
        "        if prev_acc_score - acc_score < 0.008:  # acc score does not increase\n",
        "            print(\"Exiting training due to plateau in accuracy_score\")\n",
        "            break\n",
        "\n",
        "        # break\n",
        "\n",
        "        prev_acc_score = acc_score\n",
        "\n",
        "    print(\"Loss logs \", train_loss_logs, val_loss_logs)\n",
        "    plot_losses(val_loss_logs, train_loss_logs, log_interval=log_interval)\n",
        "\n",
        "    return train_loss_logs, val_loss_logs, float(loss)\n",
        "\n",
        "\n",
        "def eval(model, test_loader, classification_threshold=0.7):\n",
        "    model.eval()\n",
        "    loss_logs = []\n",
        "    pred_sentiment = []\n",
        "    actual_sentiment = []\n",
        "    acc_score = 0.0\n",
        "    for input_text, sentiment in tqdm(test_loader):\n",
        "        output = model(list(input_text))\n",
        "        loss = F.nll_loss(output, sentiment)\n",
        "\n",
        "        if classification_threshold > 0.0:\n",
        "            output = soft_prob_assign_based_on_threshold(output, classification_threshold)\n",
        "        current_pred_sentiment = output.argmax(dim=-1)\n",
        "\n",
        "        pred_sentiment += list(current_pred_sentiment.detach().numpy())\n",
        "        actual_sentiment += list(sentiment.detach().numpy())\n",
        "\n",
        "        loss_logs.append(float(loss))\n",
        "\n",
        "    acc_score = accuracy_score(actual_sentiment, pred_sentiment)\n",
        "    print(\"Metrics: \\n\", get_classification_report(actual_sentiment, pred_sentiment))\n",
        "    plot_data(actual_sentiment=actual_sentiment, pred_sentiment=pred_sentiment)\n",
        "\n",
        "    return acc_score, sum(loss_logs) / len(loss_logs), pred_sentiment\n",
        "\n",
        "\n",
        "\n",
        "def predict(input_sentence, classification_threshold=0.7):\n",
        "    model.eval()\n",
        "    if isinstance(input_sentence, str):\n",
        "        input_sentence = [input_sentence]\n",
        "    output = model(list(input_sentence))\n",
        "    probs = torch.exp(output).gather(-1, output.argmax(dim=-1)[:, None])\n",
        "    if classification_threshold > 0.0:\n",
        "        output = soft_prob_assign_based_on_threshold(output, classification_threshold)\n",
        "\n",
        "    current_pred_sentiment = output.argmax(dim=-1)\n",
        "    pred_sentiment = list(current_pred_sentiment.detach().numpy())\n",
        "\n",
        "    sentiment_map = {\n",
        "        0: \"Negative\",  # 0 class\n",
        "        1: \"Neutral\",  # 1 class\n",
        "        2: \"Positive\",  # 2 class\n",
        "    }\n",
        "\n",
        "    print(\"Probs: \", probs.squeeze().detach().numpy())\n",
        "    print(\"Predicted: \", pred_sentiment)\n",
        "    sentiment_list = []\n",
        "    for s in pred_sentiment:\n",
        "        sentiment_list.append(sentiment_map[s])\n",
        "    print(\"sentiments: \", sentiment_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "DeWD-O1ogE_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s38_bs6UnVpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# models.py\n",
        "from transformers import AutoTokenizer, BertConfig, BertModel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = bert_model_name = \"bert-base-uncased\"\n",
        "\n",
        "\n",
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, n_classes=3, train_only_final_layer=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "        pretrained_model = BertModel.from_pretrained(bert_model_name)\n",
        "        self.model = pretrained_model\n",
        "\n",
        "        # self.model = nn.Sequential(*list(pretrained_model.children())[:-1]) # Without the last pooler layer\n",
        "        model_hidden_size = pretrained_model.config.hidden_size\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "        self.classification_layer = nn.Linear(model_hidden_size, n_classes)\n",
        "        nn.init.xavier_normal_(self.classification_layer.weight)\n",
        "\n",
        "        if train_only_final_layer:\n",
        "            self.freeze_parameters()\n",
        "\n",
        "    def freeze_parameters(self):\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input_texts):\n",
        "        tokenized_inputs = self.tokenizer(input_texts, return_tensors=\"pt\",\n",
        "                                          max_length=160, padding=True, truncation=True\n",
        "                                          )\n",
        "\n",
        "        tokenized_inputs = tokenized_inputs.to(device)\n",
        "\n",
        "        final_outputs = self.model(input_ids=tokenized_inputs.input_ids,\n",
        "                                   attention_mask=tokenized_inputs.attention_mask,\n",
        "                                   token_type_ids=tokenized_inputs.token_type_ids,\n",
        "                                   output_hidden_states=True\n",
        "                                   )\n",
        "\n",
        "        # Take the [CLS] token to predict the final layer output\n",
        "        n_dims = len(final_outputs.last_hidden_state.shape)\n",
        "\n",
        "        cls_final_embedding = None\n",
        "        if n_dims == 3:\n",
        "            cls_final_embedding = final_outputs.last_hidden_state[:, 0, :]  # B*n_tokens*hidden_size\n",
        "        else:\n",
        "            cls_final_embedding = final_outputs.last_hidden_state[0, :]\n",
        "\n",
        "        classification_logits = self.classification_layer(self.drop(cls_final_embedding))\n",
        "        res = F.log_softmax(classification_logits, dim=-1)\n",
        "        return res\n",
        "\n",
        "\n",
        "model = SentimentModel(n_classes=3, train_only_final_layer=False)"
      ],
      "metadata": {
        "id": "YqXPQmCBgIQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yUAWvLdlJO99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3vi8g_cI9DVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global params\n",
        "lr = 0.00025\n",
        "use_scheduler = True"
      ],
      "metadata": {
        "id": "F7UNurzx343q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_optimizer_scheduler(model, lr, use_scheduler=False, warmup_step=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = None\n",
        "    if use_scheduler:\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, warmup_step)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "optimizer, scheduler =  get_optimizer_scheduler(model, lr, use_scheduler=False, warmup_step=100)\n"
      ],
      "metadata": {
        "id": "l4gRaAtxgLJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2drynk0-3QKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model.load_state_dict(torch.load(\"model.pt\"))\n",
        "    optimizer.load_state_dict(torch.load(\"optim.pt\"))\n",
        "\n",
        "    # predict testing\n",
        "    input_sentence = [\"I am sad\", \"We are going to president's house\", \"I am happy\"]\n",
        "    predict(input_sentence, classification_threshold=0.7)\n",
        "\n",
        "    # train(model, optimizer, scheduler, train_loader, val_loader,\n",
        "    #       lr=0.0001, warmup_step=100,\n",
        "    #       classification_threshold=0.7, log_interval=400)\n",
        "\n",
        "\n",
        "    # Evaluate\n",
        "    acc_score, nll_loss, pred_sentiment = eval(model, test_loader, classification_threshold=0.7)\n",
        "    print(\"Eval acc_score: \", acc_score)\n",
        "\n",
        "    # count params\n",
        "    print(\"Trainable_params: \",sum([p.numel() for p in model.parameters() if p.requires_grad == True]))\n"
      ],
      "metadata": {
        "id": "wWfEZ_mI5fAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iE-J-Nt_xOXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}