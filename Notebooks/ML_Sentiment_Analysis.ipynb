{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNG7sZCrjHqh",
        "outputId": "e775b669-2635-4961-e82d-232a3419914c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"./IMDB Dataset.csv\""
      ],
      "metadata": {
        "id": "V_qMmGRbjgmr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eda\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"columns: \", df.columns)\n",
        "print(df.head())\n",
        "print(df.sentiment.value_counts)\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4e6_4APluYT",
        "outputId": "e8272f09-09fe-456b-c80b-338028c74dd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "columns:  Index(['review', 'sentiment'], dtype='object')\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "<bound method IndexOpsMixin.value_counts of 0        positive\n",
            "1        positive\n",
            "2        positive\n",
            "3        negative\n",
            "4        positive\n",
            "           ...   \n",
            "49995    positive\n",
            "49996    negative\n",
            "49997    negative\n",
            "49998    negative\n",
            "49999    negative\n",
            "Name: sentiment, Length: 50000, dtype: object>\n",
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle and create train and test df\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df_train = df[:45000]\n",
        "df_test = df[45000:]"
      ],
      "metadata": {
        "id": "rAH8hBdlyLJZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8nqq7gM2NpE",
        "outputId": "86fe4f72-8525-4b6f-852f-0797e621b8b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing and tokenizing the dataset\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "from functools import partial\n",
        "\n",
        "stopwords_list = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def tokenizer(s, type=\"l\"):\n",
        "    # remove the <br >\n",
        "    s = re.sub(\"<[^>]*>\", \" \", s)\n",
        "    # remove the https\n",
        "    s = re.sub(\"https\\S+\\s\", \" \", s)\n",
        "    words_list = word_tokenize(s)\n",
        "    # words_list = [w for w in words_list if w not in stopwords_list]\n",
        "    if type == \"l\":\n",
        "        res_words = [lemmatizer.lemmatize(w) for w in words_list]\n",
        "    else:\n",
        "        res_words = [stemmer.stem(w) for w in words_list]\n",
        "\n",
        "    return res_words\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "lemmatize_tokenizer = partial(tokenizer, type=\"l\")\n",
        "stem_tokenizer = partial(tokenizer, type=\"s\")\n",
        "\n",
        "# testing\n",
        "stem_tokenizer(\"Hi, how is it going? https:www.hello.com <br> hello <\\br>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eunMUQEPji_R",
        "outputId": "9e795a10-35e0-43ee-a347-1ce125370aee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', ',', 'how', 'is', 'it', 'go', '?', 'hello']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2hPf0NFji8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline creation\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score, recall_score, make_scorer\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"vect\", TfidfVectorizer(tokenizer=tokenizer)),\n",
        "    (\"clf\", LogisticRegression(solver='liblinear')) # as the dataset is comparatively small\n",
        "])\n",
        "\n",
        "\n",
        "param_search = [\n",
        "    {\n",
        "        \"vect__tokenizer\": [lemmatize_tokenizer,stem_tokenizer],\n",
        "        \"clf__C\": [1.0, 10.0]\n",
        "    }\n",
        "]\n",
        "\n",
        "gs = GridSearchCV(pipe, param_search, cv=5,\n",
        "                  scoring = make_scorer(f1_score, average=\"macro\"))\n",
        "\n",
        "def find_optimal_params(x_train, y_train):\n",
        "    gs.fit(x_train, y_train)\n",
        "    print(gs.best_params_)\n",
        "    best_classifier = gs.best_estimator_\n",
        "    return best_classifier\n"
      ],
      "metadata": {
        "id": "aZ3ZYT7Xji5x"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(classifier, x):\n",
        "    # res = preprocess(input_str)\n",
        "    if isinstance(x, str):\n",
        "        x = [x]\n",
        "    probs = classifier.predict_proba(x)\n",
        "    pred_sentiment = classifier.predict(x)\n",
        "    print(probs, pred_sentiment)\n",
        "    return pred_sentiment"
      ],
      "metadata": {
        "id": "T8YMd5gc0WcI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = df_train.review.values, df_train.sentiment.values\n",
        "\n",
        "best_classifier = find_optimal_params(x_train, y_train)\n",
        "classifier = best_classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDmkhlx4jixZ",
        "outputId": "5348c8ff-2eb6-4944-83a2-34a1c753cda6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clf__C': 10.0, 'vect__tokenizer': functools.partial(<function tokenizer at 0x7ada9fae3f40>, type='l')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = predict(classifier, [\"hi how are you?\", \"I am delighted to meet you\"])\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pObDOwz3Ebcc",
        "outputId": "7c3c3038-fb0f-42bb-d1fe-58a5ef0b4c59"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4890404  0.5109596 ]\n",
            " [0.34317481 0.65682519]] ['positive' 'positive']\n",
            "['positive' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "pred_sentiment = classifier.predict(df_test.review.values)\n",
        "print(classification_report(df_test.sentiment.values, pred_sentiment, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmL--sYV1A9S",
        "outputId": "7cbc3f72-cd08-460a-fa2d-dfc82c674879"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      2466\n",
            "    positive       0.89      0.91      0.90      2534\n",
            "\n",
            "    accuracy                           0.90      5000\n",
            "   macro avg       0.90      0.90      0.90      5000\n",
            "weighted avg       0.90      0.90      0.90      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg2UaQrwKOOw",
        "outputId": "aa2e4d68-ac81-4141-f0b2-9880ffb18da9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('vect',\n",
            "                 TfidfVectorizer(tokenizer=functools.partial(<function tokenizer at 0x7ada9fae3f40>, type='l'))),\n",
            "                ('clf', LogisticRegression(C=10.0, solver='liblinear'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(df_test.sentiment.values, pred_sentiment, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIVVWiJ0Lb_V",
        "outputId": "b3b35895-5634-44a1-a586-26c550c9cd0c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.909     0.886     0.897      2466\n",
            "    positive      0.892     0.914     0.903      2534\n",
            "\n",
            "    accuracy                          0.900      5000\n",
            "   macro avg      0.900     0.900     0.900      5000\n",
            "weighted avg      0.900     0.900     0.900      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BxN2Ext0ORUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}